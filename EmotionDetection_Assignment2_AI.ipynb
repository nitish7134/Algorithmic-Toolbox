{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmotionDetection_Assignment2_AI.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zqKXvJq7hNV2",
        "sl-5LlW00ZaE",
        "nan4Emg-OHno"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitish7134/Algorithmic-Toolbox/blob/master/EmotionDetection_Assignment2_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-QP2AQRzvVR"
      },
      "source": [
        "#Installing and importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWJuCpAkzvgb"
      },
      "source": [
        "!pip install tensorflow\n",
        "!pip install -U -q PyDrive\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJZ63rJfkSkW"
      },
      "source": [
        "Importing Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juQl29OqkTxq"
      },
      "source": [
        "#IMPORT ALL MODULES\n",
        "import io\n",
        "import pandas as pd\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import math\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqKXvJq7hNV2"
      },
      "source": [
        "#Utility Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl-5LlW00ZaE"
      },
      "source": [
        "## NN using **TensorFlow** and **Keras**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5RuFsMO0crz"
      },
      "source": [
        "Model Define:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-dzt8st0f5C"
      },
      "source": [
        "def defineModel(inputSize,layer_dims=[100,50,25,12,6,3],learningRate=0.001):\n",
        "  input_layer = tf.keras.Input(shape=(inputSize,))\n",
        "  prevLayer = input_layer\n",
        "  for l in range(len(layer_dims)):\n",
        "    hiddenLayer = Dense(layer_dims[l],activation='relu')(prevLayer)\n",
        "    prevLayer = hiddenLayer \n",
        "  output_layer = Dense(1,activation='sigmoid')(prevLayer)\n",
        "  model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "  optimiser = tf.keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
        "\n",
        "  model.compile(optimizer=optimiser, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2122GhC5hFF"
      },
      "source": [
        "def trainModel(model,X,Y,epochs=150,batch_size=1024,verbose=0):\n",
        "  model.fit(X,Y,epochs=epochs, batch_size=batch_size,shuffle=True, verbose=verbose)\n",
        "  loss, acc = model.evaluate(X, Y, verbose=verbose)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IZDANar7J4B"
      },
      "source": [
        "def predictNN(model,x,verbose=0):\n",
        "  predictions = model.predict(x=x,verbose=verbose)\n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bQjPCCtdFkp"
      },
      "source": [
        "##GA Utility Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMi7vNaBgk-N"
      },
      "source": [
        "Filter Features according to chromosome\n",
        "Parameters : \n",
        "\n",
        "> Chromosome (A string of 0 or 1 where 1 means feature at index i is to be considered.)\n",
        "\n",
        "> Features (Input array containing all the features)\n",
        "\n",
        "\n",
        "\n",
        "Return :\n",
        "> Filtered Features.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9F7nFkNdKbN"
      },
      "source": [
        "\n",
        "def filterFeatures(chromosome,features):\n",
        "  selected_elements_indices = np.where(chromosome == 1)[0]\n",
        "  filteredFeatures = features[:, selected_elements_indices]\n",
        "  return filteredFeatures\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFJwL1-jhDQy"
      },
      "source": [
        "Find Fitness of the population\n",
        "Parameters : \n",
        "\n",
        "\n",
        "> Chromosome : Chromosome for which fitness is to be calculated.\n",
        "\n",
        "\n",
        "> train_X, train_Y, test_X, test_Y : Dataset to check accuracy of NN for fitness function\n",
        "\n",
        "Return : \n",
        "> Fitness of the chromosome\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUNEdHTPgkJd"
      },
      "source": [
        "def fitnessFunction(chromosome, train_X, train_Y, test_X, test_Y):\n",
        "  trainData = filterFeatures(chromosome,train_X)\n",
        "  testData = filterFeatures(chromosome,test_X)\n",
        "  model = defineModel(inputSize = trainData.shape[1])\n",
        "  trainModel(model,trainData,train_Y)\n",
        "\n",
        "  result = model.evaluate(testData, test_Y,batch_size=32)\n",
        "  accuracy = result[1]\n",
        "  loss = result[0]\n",
        "  fitness = accuracy-(trainData.shape[1]/(len(chromosome)*10) + loss/1000)\n",
        "  return fitness\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHYKtJMf2BfD"
      },
      "source": [
        "Crossver of two Parents:\n",
        "Parameters :\n",
        "> parent1, parent2 : Two Chromosomes\n",
        "\n",
        "> Method : 0 or 1 \n",
        "\n",
        "\n",
        "\n",
        "Returns :\n",
        "> Offspings Of the two Chromosomes\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-jKgqSI2CVP"
      },
      "source": [
        "def GAcrossOver(parent1,parent2,method):\n",
        "  if method == 0:\n",
        "    r1 = random.randint(0,parent1.shape[0]-1)\n",
        "    return np.append(parent1[0:r1],parent2[r1:]),np.append(parent2[0:r1],parent1[r1:])\n",
        "  elif method == 1:\n",
        "    r1 = random.randint(0,parent1.shape[0]-1)\n",
        "    r2 = random.randint(r1+1,parent1.shape[0]-1)\n",
        "    offspring1 = np.append(np.append(parent1[0:r1],parent2[r1:r2]),parent1[r2:])\n",
        "    offspring2 = np.append(np.append(parent2[0:r1],parent1[r1:r2]),parent2[r2:]) \n",
        "    return offspring1,offspring2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2e5o-2h3viF"
      },
      "source": [
        "Mutation of a chromosome\n",
        "Parameters: \n",
        "> chromosome : Chromosome to be mutated.\n",
        "\n",
        "> Method : 0 or 1\n",
        "\n",
        "Return : \n",
        "\n",
        "> Mutated Chromosome\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4k1oYJ54V3C"
      },
      "source": [
        "def GAmutate(chromosome,method):\n",
        "  if method == 0:\n",
        "    temp = np.random.randint(2,size=chromosome.shape[0])\n",
        "    chromosome = np.logical_xor(chromosome,temp)\n",
        "  elif method ==1:\n",
        "    r1 = random.randint(0,chromosome.shape[0]-1)\n",
        "    chromosome = np.append(chromosome[0:r1],np.logical_not(chromosome[r1:]))\n",
        "  return chromosome"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moqZIeHV6c12"
      },
      "source": [
        "Initial Population:\n",
        "Parameters: \n",
        "\n",
        "> no_of_features \n",
        "\n",
        "> populationSize\n",
        " \n",
        "Return:\n",
        "  Randomly Generated population\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFKDeiho6x7I"
      },
      "source": [
        "def initialPopulation(no_of_features,populationSize):\n",
        "  x = np.random.randint(2,size=(populationSize,no_of_features-1))\n",
        "  y = np.ones(shape=(populationSize,1))\n",
        "  chromosomePop = np.append(x, y, axis=1)\n",
        "  return chromosomePop\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5gHrplieS7h"
      },
      "source": [
        "#Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87sbSQJ2rZfa"
      },
      "source": [
        "Auth your Gdrive to read compiled csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUDsfZYhn-Me"
      },
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_FOPqPHrhiU"
      },
      "source": [
        "Update links for the csv file to read the csv files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR8JnxTXpuPa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "78900641-794e-4307-f9a2-b582f9e25bd9"
      },
      "source": [
        "## ADD LINKS TO DRIVE CSV FILES\n",
        "link = \"https://drive.google.com/file/d/1GnYwSQvAFE3X4TVYrO_PldUkhT0xB6w0/view\"  ##LINK FOR FEAR CSV\n",
        "link2= \"https://drive.google.com/file/d/1f97QxMIiRHVDqiflbM09Y0rbMOY3CI7F/view\" ## LINK FOR NOT FEAR CSV\n",
        "id = link.split(\"/\")[-2] \n",
        "id2 = link2.split(\"/\")[-2] \n",
        "\n",
        "downloaded = drive.CreateFile({'id':id})  \n",
        "downloaded2 = drive.CreateFile({'id':id2})  \n",
        "\n",
        "downloaded.GetContentFile('fear.csv')   \n",
        "downloaded2.GetContentFile('noFear.csv')\n",
        "\n",
        "fearData = pd.read_csv('fear.csv') \n",
        "noFearData = pd.read_csv('noFear.csv')\n",
        "fearData = fearData.drop(fearData.columns[0], axis=1)\n",
        "noFearData = noFearData.drop(noFearData.columns[0], axis=1)\n",
        "\n",
        "fearArr = fearData.to_numpy()\n",
        "noFearArr = noFearData.to_numpy()\n",
        "  \n",
        "fear = np.ones((len(fearArr), 1))\n",
        "noFear = np.zeros((len(noFearArr), 1))\n",
        "\n",
        "mergedArrX = np.concatenate((fearArr,noFearArr),axis=0)\n",
        "mergedArrY = np.concatenate((fear,noFear),axis=0)\n",
        "#print(mergedArrX)\n",
        "#print(mergedArrY.shape)\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(mergedArrX, mergedArrY, test_size = 0.2, random_state = 0)\n",
        "xTrain = tf.keras.utils.normalize(xTrain)\n",
        "yTrain = tf.keras.utils.normalize(yTrain)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ApiRequestError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydrive/files.py\u001b[0m in \u001b[0;36mFetchMetadata\u001b[0;34m(self, fields, fetch_all)\u001b[0m\n\u001b[1;32m    236\u001b[0m                                                  fields=fields)\\\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHttpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHttpError\u001b[0m: <HttpError 404 when requesting https://www.googleapis.com/drive/v2/files/1GnYwSQvAFE3X4TVYrO_PldUkhT0xB6w0?alt=json returned \"File not found: 1GnYwSQvAFE3X4TVYrO_PldUkhT0xB6w0\">",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mApiRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-3c61ef94f06a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdownloaded2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCreateFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mid2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdownloaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetContentFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fear.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdownloaded2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetContentFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'noFear.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydrive/files.py\u001b[0m in \u001b[0;36mGetContentFile\u001b[0;34m(self, filename, mimetype, remove_bom)\u001b[0m\n\u001b[1;32m    208\u001b[0m                     \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_bom\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mremove_bom\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFetchContent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmimetype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_bom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydrive/files.py\u001b[0m in \u001b[0;36m_decorated\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_decorated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFetchMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecoratee\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_decorated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydrive/auth.py\u001b[0m in \u001b[0;36m_decorated\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGet_Http_Object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecoratee\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_decorated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydrive/files.py\u001b[0m in \u001b[0;36mFetchMetadata\u001b[0;34m(self, fields, fetch_all)\u001b[0m\n\u001b[1;32m    237\u001b[0m           \u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHttpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mApiRequestError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mApiRequestError\u001b[0m: <HttpError 404 when requesting https://www.googleapis.com/drive/v2/files/1GnYwSQvAFE3X4TVYrO_PldUkhT0xB6w0?alt=json returned \"File not found: 1GnYwSQvAFE3X4TVYrO_PldUkhT0xB6w0\">"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAi5GtL_eY2n"
      },
      "source": [
        "#Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5ZDYkesLt6F"
      },
      "source": [
        "##Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUk46iIt_s5R"
      },
      "source": [
        "Enter Hyperparameters:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj_GFSH4_wcQ"
      },
      "source": [
        "PopulationSize = 10\n",
        "NumberOfGenerations = 10\n",
        "MutationRate = 0.05\n",
        "CrossoverRate = 0.8\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO6YiKJcHcLt"
      },
      "source": [
        "Fitness Utility:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NALMeoDRHbml"
      },
      "source": [
        "def getFitness(chromosome):\n",
        "  return fitnessFunction(chromosome,xTrain, yTrain, xTest, yTest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_xtiIBm9jHX"
      },
      "source": [
        "import time\n",
        "startTime = time.time()\n",
        "chromosomePop = initialPopulation(xTrain.shape[1],PopulationSize)\n",
        "population = list()\n",
        "for chromosome in chromosomePop:\n",
        "  citizen = list()\n",
        "  citizen.append(getFitness(chromosome))\n",
        "  citizen.append(chromosome)\n",
        "  population.append(citizen)\n",
        "population = sorted(population,key=lambda x : x[0],reverse=True) \n",
        "print(\"Time Taken to create population\")\n",
        "totalTime = time.time()-startTime\n",
        "print(totalTime)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewCWzaBEuBfv"
      },
      "source": [
        "sTime = time.time()\n",
        "print(\"Generation 0: \")\n",
        "print(\"\")\n",
        "print('Best Fitness: {}'.format(population[0][0]))\n",
        "for gen in range(1,NumberOfGenerations+1):\n",
        "  newPopulation = list();\n",
        "  BestCitizen = population[0]\n",
        "  #Crossover\n",
        "  for i in range(int(CrossoverRate*PopulationSize)):\n",
        "    r1 = random.randint(0,PopulationSize-1)\n",
        "    r2 = random.randint(0,PopulationSize-1)\n",
        "    offspring1,offspring2 = GAcrossOver(population[r1][1],population[r2][1],random.randint(0,1))\n",
        "    tempList = list()\n",
        "    tempList.append(population[r1])\n",
        "    tempList.append(population[r2])\n",
        "\n",
        "    citizen1 = list()\n",
        "    citizen1.append(getFitness(offspring1))\n",
        "    citizen1.append(offspring1)\n",
        "    tempList.append(citizen1)\n",
        "\n",
        "    citizen2 = list()\n",
        "    citizen2.append(getFitness(offspring2))\n",
        "    citizen2.append(offspring2)\n",
        "    tempList.append(citizen2)\n",
        "\n",
        "    tempList = sorted(tempList,key=lambda x : x[0],reverse=True) \n",
        "\n",
        "    newPopulation.append(tempList[0])\n",
        "    newPopulation.append(tempList[1])\n",
        "\n",
        "  #Add Remaining\n",
        "  for i in range(PopulationSize-len(newPopulation)):\n",
        "    r1 = random.randint(0,PopulationSize-1)\n",
        "    r2 = random.randint(0,PopulationSize-1)\n",
        "    newPopulation.append(population[0])\n",
        "    newPopulation.append(population[1])\n",
        "\n",
        "  #Mutation\n",
        "  for i in range(int(MutationRate*PopulationSize)):\n",
        "    r1 = random.randint(0,PopulationSize-1)\n",
        "    offspring = GAmutate(newPopulation[r1][1],random.randint(0,1))\n",
        "    newFitness = getFitness(offspring)\n",
        "    newPopulation[r1] = [newFitness,offspring]\n",
        "\n",
        "  population = sorted(newPopulation,key=lambda x : x[0],reverse=True) \n",
        "  if(population[0][0]<BestCitizen[0]):\n",
        "    population[0] = BestCitizen\n",
        "  print('Generation : {}'.format(gen))\n",
        "  print('')\n",
        "  print('Best Fitness: {}'.format(population[0][0]))\n",
        "  \n",
        "endTime= time.time()\n",
        "print(\"time taken to process all generations:\")\n",
        "timetaken = endTime-sTime\n",
        "print(timetaken)\n",
        "BestChromosome = population[0][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUmP6nNGLjoo"
      },
      "source": [
        "\n",
        "##Neural Netowork After Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tL9hpuwtNHY0",
        "outputId": "e606c749-a8be-49cd-f170-303234185e30"
      },
      "source": [
        "trainData = filterFeatures(BestChromosome,xTrain)\n",
        "testData = filterFeatures(BestChromosome,xTest)\n",
        "print(\"No of Final Features:\")\n",
        "print(trainData.shape[1])\n",
        "print(\"No of Models to train on:\")\n",
        "print(trainData.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of Final Features:\n",
            "311\n",
            "No of Models to train on:\n",
            "1509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deIdbND4LpQl"
      },
      "source": [
        "Enter Hyperparameters:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fURRCAYmL_FU"
      },
      "source": [
        "layerDims = [450,300,200,130,85,60,40,25,15,10,5,3,1]\n",
        "epochs=100\n",
        "batch_size=1024\n",
        "LearningRate = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_LFlFkOLotO",
        "outputId": "15ec9788-fd7a-4f77-b7c2-beedf808d550"
      },
      "source": [
        "model = defineModel(trainData.shape[1],layerDims,LearningRate)\n",
        "trainModel(model,trainData,yTrain,epochs=epochs,batch_size=batch_size,verbose=0)\n",
        "\n",
        "print(\"Train Result\")\n",
        "results = model.evaluate(trainData, yTrain, batch_size=batch_size,verbose=0)\n",
        "print(results)\n",
        "print('Training Done')  \n",
        "print(\"Test Result\")\n",
        "results = model.evaluate(testData, yTest, batch_size=batch_size,verbose=0)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Result\n",
            "[0.6084244251251221, 0.9655401110649109]\n",
            "Training Done\n",
            "Test Result\n",
            "[0.6134869456291199, 0.9391534328460693]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoJNm8OIFtgt"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LB-nV9dvK_t0",
        "outputId": "d766acf2-e198-43ad-84af-e173f1f93dec"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier_knn = KNeighborsClassifier(n_neighbors = 4)\n",
        "classifier_knn.fit(trainData,yTrain)\n",
        "#y_pred = classifier_knn.predict(x_test)\n",
        "trainaccuracy = classifier_knn.score(trainData,yTrain)\n",
        "print(trainaccuracy)\n",
        "\n",
        "accuracy = classifier_knn.score(testData,yTest)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9854208084824387\n",
            "0.8677248677248677\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nan4Emg-OHno"
      },
      "source": [
        "##To Predict on given image:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fccFEX30OXLP"
      },
      "source": [
        "#Get Value for input from the Compiled CSV: "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "CuDGhnPd5Xdl",
        "outputId": "ff4f0ee2-cc1e-4d5b-bc8e-ed60d2c9ede0"
      },
      "source": [
        "predictions = predictNN(model,input)\n",
        "for i in range(len(predictions)):\n",
        "  print('The person in image with Index  {} is :'.format(i))\n",
        "  if predictions[i] == 1:\n",
        "    print(\"In Fear\")\n",
        "  else:\n",
        "    print(\"Not In Fear\")\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-b714da4aa84a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The person in image with Index  {} is :'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"In Fear\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-92-33807a599ab8>\u001b[0m in \u001b[0;36mpredictNN\u001b[0;34m(model, x, verbose)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredictNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1577\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m     self._adapter = adapter_cls(\n\u001b[1;32m   1106\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    969\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 971\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    972\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m     raise RuntimeError(\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'method'>, <class 'NoneType'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9nj0oePz_0H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}